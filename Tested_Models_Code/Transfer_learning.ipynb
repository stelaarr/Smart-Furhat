{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\anaconda3\\envs\\iis\\lib\\site-packages\\kornia\\feature\\lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import opencv_jupyter_ui as jcv2\n",
    "from feat import Detector\n",
    "from IPython.display import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Test Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "\n",
    "# Define image transforms (resizing, normalization, etc.)\n",
    "transform = transforms.Compose([ \n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),  # Resize\n",
    "    transforms.ToTensor(),          \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# FER\n",
    "data_path_fer = r'C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\data\\FER2013\\train' \n",
    "test_data_path_fer = r'C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\data\\FER2013\\test'\n",
    "# RAF\n",
    "train_data_path_raf = r'C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\data\\RAF-DB\\train'  \n",
    "test_data_path_raf = r'C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\data\\RAF-DB\\test' \n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(data_path):\n",
    "    dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    return dataset\n",
    "\n",
    "# Load the datasets\n",
    "full_dataset = load_dataset(data_path_fer)\n",
    "test_dataset = load_dataset(test_data_path_fer)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# split the dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "# DataLoader for batching and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"Train Classes: {full_dataset.classes}\")  \n",
    "print(f\"Test Classes: {test_dataset.classes}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Test Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets RAF\n",
    "full_dataset_raf = load_dataset(data_path_fer)\n",
    "test_dataset_raf = load_dataset(test_data_path_fer)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset_raf))\n",
    "val_size = len(full_dataset_raf) - train_size\n",
    "\n",
    "# split the dataset\n",
    "train_dataset_raf, val_dataset_raf = random_split(full_dataset_raf, [train_size, val_size])\n",
    "\n",
    "\n",
    "#  DataLoader for batching and shuffling\n",
    "train_loader_raf = DataLoader(train_dataset_raf, batch_size=32, shuffle=True)\n",
    "val_loader_raf = DataLoader(val_dataset_raf, batch_size=32, shuffle=False)\n",
    "test_loader_raf = DataLoader(test_dataset_raf, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"Train Classes: {full_dataset_raf.classes}\")  \n",
    "print(f\"Test Classes: {test_dataset_raf.classes}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AffectNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['angry', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Number of training samples: 37553\n",
      "Number of validation samples: 800\n",
      "Number of test samples: 3200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define data paths\n",
    "data_dir = r\"C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\data\\AffectNet\"  \n",
    "train_dir = f\"{data_dir}/train\"\n",
    "val_dir = f\"{data_dir}/val\"\n",
    "test_dir = f\"{data_dir}/test\"\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resizing \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms[\"train\"])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms[\"val\"])\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms[\"val\"])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "# Check class names and dataset sizes\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detector backend: opencv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with opencv: 100%|██████████| 225/225 [12:12<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with opencv: 50.28%\n",
      "Evaluating detector backend: mtcnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with mtcnn: 100%|██████████| 225/225 [38:57<00:00, 10.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with mtcnn: 52.97%\n",
      "Evaluating detector backend: centerface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with centerface: 100%|██████████| 225/225 [28:31<00:00,  7.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with centerface: 55.73%\n",
      "Evaluating detector backend: mediapipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with mediapipe: 100%|██████████| 225/225 [05:08<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with mediapipe: 45.81%\n",
      "\n",
      "Final Results:\n",
      "opencv: 50.28%\n",
      "mtcnn: 52.97%\n",
      "centerface: 55.73%\n",
      "mediapipe: 45.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Some detector backends to test\n",
    "detector_backends = ['opencv', 'mtcnn', 'centerface', 'mediapipe']\n",
    "\n",
    "class_labels = full_dataset.classes  \n",
    "\n",
    "def evaluate_detector_backend(test_loader, detector_backend):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a specific detector backend.\n",
    "    \n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    print(f\"Evaluating detector backend: {detector_backend}\")\n",
    "    for images, labels in tqdm(test_loader, desc=f\"Processing with {detector_backend}\"):\n",
    "        for i in range(images.size(0)): \n",
    "            # Convert tensor to a NumPy array \n",
    "            img = images[i].cpu().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
    "            img = ((img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]) * 255  # De-normalize\n",
    "            img = img.astype('uint8') \n",
    "\n",
    "            try:\n",
    "                # Predict emotion using the specified backend\n",
    "                analysis = DeepFace.analyze(img_path=img, actions=['emotion'], detector_backend=detector_backend, enforce_detection=False)\n",
    "                predicted_emotion = analysis[0]['dominant_emotion']\n",
    "\n",
    "                true_labels.append(class_labels[labels[i].item()]) \n",
    "                predicted_labels.append(predicted_emotion.lower())\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels) * 100\n",
    "    print(f\"Accuracy with {detector_backend}: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# hyperparameter search\n",
    "results = {}\n",
    "for backend in detector_backends:\n",
    "    accuracy = evaluate_detector_backend(test_loader, detector_backend=backend)\n",
    "    results[backend] = accuracy\n",
    "\n",
    "# overall results\n",
    "print(\"\\nFinal Results:\")\n",
    "for backend, acc in results.items():\n",
    "    print(f\"{backend}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valuate the model on the test set\n",
    "def evaluate_model(model, loader):\n",
    "    print('------- Evaluating -------')\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    print('------------ Training ----------------')\n",
    "    best_val_accuracy = 0.0 \n",
    "    best_model_path = \"best_resnet18.pth\"  # save the best model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "         # Validation phase\n",
    "        model.eval() \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Track validation accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Save the model if it has the best validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"Training complete. Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Training ----------------\n",
      "Epoch [1/30], Loss: 1.6279, Accuracy: 36.05%\n",
      "Validation Loss: 1.5121, Validation Accuracy: 41.27%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 41.27%\n",
      "Epoch [2/30], Loss: 1.5156, Accuracy: 41.50%\n",
      "Validation Loss: 1.5067, Validation Accuracy: 40.98%\n",
      "--------------------------------------------------\n",
      "Epoch [3/30], Loss: 1.4878, Accuracy: 43.22%\n",
      "Validation Loss: 1.4734, Validation Accuracy: 43.10%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.10%\n",
      "Epoch [4/30], Loss: 1.4787, Accuracy: 43.34%\n",
      "Validation Loss: 1.5137, Validation Accuracy: 41.17%\n",
      "--------------------------------------------------\n",
      "Epoch [5/30], Loss: 1.4682, Accuracy: 43.79%\n",
      "Validation Loss: 1.4963, Validation Accuracy: 43.23%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.23%\n",
      "Epoch [6/30], Loss: 1.4605, Accuracy: 43.99%\n",
      "Validation Loss: 1.4544, Validation Accuracy: 43.80%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.80%\n",
      "Epoch [7/30], Loss: 1.4598, Accuracy: 44.22%\n",
      "Validation Loss: 1.4427, Validation Accuracy: 44.18%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 44.18%\n",
      "Epoch [8/30], Loss: 1.4536, Accuracy: 44.35%\n",
      "Validation Loss: 1.4372, Validation Accuracy: 43.99%\n",
      "--------------------------------------------------\n",
      "Epoch [9/30], Loss: 1.4442, Accuracy: 44.83%\n",
      "Validation Loss: 1.4330, Validation Accuracy: 44.91%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 44.91%\n",
      "Epoch [10/30], Loss: 1.4428, Accuracy: 44.89%\n",
      "Validation Loss: 1.4390, Validation Accuracy: 45.02%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 45.02%\n",
      "Epoch [11/30], Loss: 1.4418, Accuracy: 44.60%\n",
      "Validation Loss: 1.4296, Validation Accuracy: 44.88%\n",
      "--------------------------------------------------\n",
      "Epoch [12/30], Loss: 1.4400, Accuracy: 45.16%\n",
      "Validation Loss: 1.4268, Validation Accuracy: 45.47%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 45.47%\n",
      "Epoch [13/30], Loss: 1.4431, Accuracy: 44.51%\n",
      "Validation Loss: 1.4377, Validation Accuracy: 44.74%\n",
      "--------------------------------------------------\n",
      "Epoch [14/30], Loss: 1.4341, Accuracy: 45.25%\n",
      "Validation Loss: 1.4796, Validation Accuracy: 42.51%\n",
      "--------------------------------------------------\n",
      "Epoch [15/30], Loss: 1.4318, Accuracy: 45.50%\n",
      "Validation Loss: 1.4791, Validation Accuracy: 43.42%\n",
      "--------------------------------------------------\n",
      "Epoch [16/30], Loss: 1.4385, Accuracy: 45.33%\n",
      "Validation Loss: 1.4661, Validation Accuracy: 43.78%\n",
      "--------------------------------------------------\n",
      "Epoch [17/30], Loss: 1.4328, Accuracy: 45.10%\n",
      "Validation Loss: 1.4519, Validation Accuracy: 43.71%\n",
      "--------------------------------------------------\n",
      "Epoch [18/30], Loss: 1.4312, Accuracy: 45.10%\n",
      "Validation Loss: 1.4412, Validation Accuracy: 45.26%\n",
      "--------------------------------------------------\n",
      "Epoch [19/30], Loss: 1.4301, Accuracy: 45.53%\n",
      "Validation Loss: 1.4697, Validation Accuracy: 42.89%\n",
      "--------------------------------------------------\n",
      "Epoch [20/30], Loss: 1.4313, Accuracy: 45.39%\n",
      "Validation Loss: 1.5035, Validation Accuracy: 42.86%\n",
      "--------------------------------------------------\n",
      "Epoch [21/30], Loss: 1.4316, Accuracy: 45.29%\n",
      "Validation Loss: 1.4617, Validation Accuracy: 44.29%\n",
      "--------------------------------------------------\n",
      "Epoch [22/30], Loss: 1.4331, Accuracy: 45.31%\n",
      "Validation Loss: 1.4385, Validation Accuracy: 44.69%\n",
      "--------------------------------------------------\n",
      "Epoch [23/30], Loss: 1.4239, Accuracy: 45.66%\n",
      "Validation Loss: 1.4526, Validation Accuracy: 44.34%\n",
      "--------------------------------------------------\n",
      "Epoch [24/30], Loss: 1.4297, Accuracy: 45.02%\n",
      "Validation Loss: 1.4728, Validation Accuracy: 43.63%\n",
      "--------------------------------------------------\n",
      "Epoch [25/30], Loss: 1.4240, Accuracy: 45.69%\n",
      "Validation Loss: 1.4348, Validation Accuracy: 44.93%\n",
      "--------------------------------------------------\n",
      "Epoch [26/30], Loss: 1.4284, Accuracy: 45.58%\n",
      "Validation Loss: 1.4762, Validation Accuracy: 43.77%\n",
      "--------------------------------------------------\n",
      "Epoch [27/30], Loss: 1.4290, Accuracy: 45.20%\n",
      "Validation Loss: 1.4334, Validation Accuracy: 45.28%\n",
      "--------------------------------------------------\n",
      "Epoch [28/30], Loss: 1.4262, Accuracy: 45.47%\n",
      "Validation Loss: 1.4346, Validation Accuracy: 44.81%\n",
      "--------------------------------------------------\n",
      "Epoch [29/30], Loss: 1.4241, Accuracy: 45.82%\n",
      "Validation Loss: 1.4477, Validation Accuracy: 44.48%\n",
      "--------------------------------------------------\n",
      "Epoch [30/30], Loss: 1.4307, Accuracy: 45.43%\n",
      "Validation Loss: 1.5076, Validation Accuracy: 41.36%\n",
      "--------------------------------------------------\n",
      "Training complete. Best validation accuracy: 45.47%\n",
      "------- Evaluating -------\n",
      "40.9445528002229\n",
      "Test Accuracy: 40.94%\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50, ResNet18_Weights\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Load pretrained ResNet18\n",
    "resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Replace the final fully connected layer\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, len(full_dataset.classes))  # Align with number of emotion classes\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "train_model(resnet, train_loader, val_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "# Evaluate on the test set after training\n",
    "test_accuracy = evaluate_model(resnet, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(resnet.state_dict(), \"emotion_recognition_resnet18.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Training ----------------\n",
      "Epoch [1/30], Loss: 1.6196, Accuracy: 36.45%\n",
      "Validation Loss: 1.5706, Validation Accuracy: 39.66%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 39.66%\n",
      "Epoch [2/30], Loss: 1.5176, Accuracy: 41.36%\n",
      "Validation Loss: 1.5317, Validation Accuracy: 41.27%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 41.27%\n",
      "Epoch [3/30], Loss: 1.4897, Accuracy: 42.87%\n",
      "Validation Loss: 1.5142, Validation Accuracy: 41.61%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 41.61%\n",
      "Epoch [4/30], Loss: 1.4704, Accuracy: 43.24%\n",
      "Validation Loss: 1.4876, Validation Accuracy: 43.52%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.52%\n",
      "Epoch [5/30], Loss: 1.4677, Accuracy: 43.25%\n",
      "Validation Loss: 1.4938, Validation Accuracy: 42.08%\n",
      "--------------------------------------------------\n",
      "Epoch [6/30], Loss: 1.4519, Accuracy: 44.34%\n",
      "Validation Loss: 1.5059, Validation Accuracy: 41.76%\n",
      "--------------------------------------------------\n",
      "Epoch [7/30], Loss: 1.4537, Accuracy: 44.02%\n",
      "Validation Loss: 1.4781, Validation Accuracy: 42.74%\n",
      "--------------------------------------------------\n",
      "Epoch [8/30], Loss: 1.4523, Accuracy: 44.24%\n",
      "Validation Loss: 1.5047, Validation Accuracy: 42.42%\n",
      "--------------------------------------------------\n",
      "Epoch [9/30], Loss: 1.4427, Accuracy: 44.57%\n",
      "Validation Loss: 1.5057, Validation Accuracy: 42.79%\n",
      "--------------------------------------------------\n",
      "Epoch [10/30], Loss: 1.4417, Accuracy: 44.72%\n",
      "Validation Loss: 1.5168, Validation Accuracy: 42.09%\n",
      "--------------------------------------------------\n",
      "Epoch [11/30], Loss: 1.4472, Accuracy: 44.24%\n",
      "Validation Loss: 1.4923, Validation Accuracy: 43.59%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.59%\n",
      "Epoch [12/30], Loss: 1.4338, Accuracy: 44.58%\n",
      "Validation Loss: 1.4860, Validation Accuracy: 43.52%\n",
      "--------------------------------------------------\n",
      "Epoch [13/30], Loss: 1.4425, Accuracy: 44.86%\n",
      "Validation Loss: 1.4794, Validation Accuracy: 43.75%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.75%\n",
      "Epoch [14/30], Loss: 1.4306, Accuracy: 45.38%\n",
      "Validation Loss: 1.4905, Validation Accuracy: 43.61%\n",
      "--------------------------------------------------\n",
      "Epoch [15/30], Loss: 1.4375, Accuracy: 44.96%\n",
      "Validation Loss: 1.5200, Validation Accuracy: 41.41%\n",
      "--------------------------------------------------\n",
      "Epoch [16/30], Loss: 1.4341, Accuracy: 45.28%\n",
      "Validation Loss: 1.5404, Validation Accuracy: 42.28%\n",
      "--------------------------------------------------\n",
      "Epoch [17/30], Loss: 1.4377, Accuracy: 45.12%\n",
      "Validation Loss: 1.4764, Validation Accuracy: 43.17%\n",
      "--------------------------------------------------\n",
      "Epoch [18/30], Loss: 1.4294, Accuracy: 45.20%\n",
      "Validation Loss: 1.4686, Validation Accuracy: 43.71%\n",
      "--------------------------------------------------\n",
      "Epoch [19/30], Loss: 1.4246, Accuracy: 45.78%\n",
      "Validation Loss: 1.4888, Validation Accuracy: 43.16%\n",
      "--------------------------------------------------\n",
      "Epoch [20/30], Loss: 1.4287, Accuracy: 45.35%\n",
      "Validation Loss: 1.5169, Validation Accuracy: 40.84%\n",
      "--------------------------------------------------\n",
      "Epoch [21/30], Loss: 1.4243, Accuracy: 45.23%\n",
      "Validation Loss: 1.5131, Validation Accuracy: 41.92%\n",
      "--------------------------------------------------\n",
      "Epoch [22/30], Loss: 1.4279, Accuracy: 45.40%\n",
      "Validation Loss: 1.4909, Validation Accuracy: 42.34%\n",
      "--------------------------------------------------\n",
      "Epoch [23/30], Loss: 1.4292, Accuracy: 45.00%\n",
      "Validation Loss: 1.4942, Validation Accuracy: 42.34%\n",
      "--------------------------------------------------\n",
      "Epoch [24/30], Loss: 1.4273, Accuracy: 45.25%\n",
      "Validation Loss: 1.4986, Validation Accuracy: 43.23%\n",
      "--------------------------------------------------\n",
      "Epoch [25/30], Loss: 1.4341, Accuracy: 44.83%\n",
      "Validation Loss: 1.5035, Validation Accuracy: 42.62%\n",
      "--------------------------------------------------\n",
      "Epoch [26/30], Loss: 1.4301, Accuracy: 45.37%\n",
      "Validation Loss: 1.4852, Validation Accuracy: 43.14%\n",
      "--------------------------------------------------\n",
      "Epoch [27/30], Loss: 1.4272, Accuracy: 45.25%\n",
      "Validation Loss: 1.5277, Validation Accuracy: 43.26%\n",
      "--------------------------------------------------\n",
      "Epoch [28/30], Loss: 1.4304, Accuracy: 45.43%\n",
      "Validation Loss: 1.5242, Validation Accuracy: 41.00%\n",
      "--------------------------------------------------\n",
      "Epoch [29/30], Loss: 1.4247, Accuracy: 45.51%\n",
      "Validation Loss: 1.5187, Validation Accuracy: 41.43%\n",
      "--------------------------------------------------\n",
      "Epoch [30/30], Loss: 1.4324, Accuracy: 45.14%\n",
      "Validation Loss: 1.4790, Validation Accuracy: 42.93%\n",
      "--------------------------------------------------\n",
      "Training complete. Best validation accuracy: 43.75%\n",
      "------- Evaluating -------\n",
      "43.07606575647813\n",
      "Test Accuracy: 43.08%\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pretrained ResNet34\n",
    "resnet34 = models.resnet34(weights='DEFAULT')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Freeze the convolutional layers\n",
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Replace the final fully connected layer\n",
    "resnet34.fc = nn.Linear(resnet34.fc.in_features, len(full_dataset.classes))  # Align with number of emotion classes\n",
    "resnet34 = resnet34.to(device)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet34.parameters(), lr=0.001)\n",
    "\n",
    "train_model(resnet34, train_loader, val_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "# Evaluate on the test set after training\n",
    "test_accuracy_34 = evaluate_model(resnet34, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy_34:.2f}%\")\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(resnet34.state_dict(), \"emotion_recognition_resnet34.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Training ----------------\n",
      "Epoch [1/30], Loss: 1.6198, Accuracy: 36.64%\n",
      "Validation Loss: 1.5435, Validation Accuracy: 42.02%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 42.02%\n",
      "Epoch [2/30], Loss: 1.5463, Accuracy: 40.77%\n",
      "Validation Loss: 1.5446, Validation Accuracy: 40.46%\n",
      "--------------------------------------------------\n",
      "Epoch [3/30], Loss: 1.5307, Accuracy: 41.32%\n",
      "Validation Loss: 1.5140, Validation Accuracy: 41.94%\n",
      "--------------------------------------------------\n",
      "Epoch [4/30], Loss: 1.5309, Accuracy: 40.78%\n",
      "Validation Loss: 1.5130, Validation Accuracy: 41.68%\n",
      "--------------------------------------------------\n",
      "Epoch [5/30], Loss: 1.5331, Accuracy: 41.35%\n",
      "Validation Loss: 1.4974, Validation Accuracy: 42.49%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 42.49%\n",
      "Epoch [6/30], Loss: 1.5190, Accuracy: 41.79%\n",
      "Validation Loss: 1.4884, Validation Accuracy: 42.51%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 42.51%\n",
      "Epoch [7/30], Loss: 1.5205, Accuracy: 41.80%\n",
      "Validation Loss: 1.4977, Validation Accuracy: 41.48%\n",
      "--------------------------------------------------\n",
      "Epoch [8/30], Loss: 1.5228, Accuracy: 42.00%\n",
      "Validation Loss: 1.4737, Validation Accuracy: 43.14%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.14%\n",
      "Epoch [9/30], Loss: 1.5208, Accuracy: 41.82%\n",
      "Validation Loss: 1.4743, Validation Accuracy: 43.43%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.43%\n",
      "Epoch [10/30], Loss: 1.5312, Accuracy: 41.70%\n",
      "Validation Loss: 1.5706, Validation Accuracy: 40.04%\n",
      "--------------------------------------------------\n",
      "Epoch [11/30], Loss: 1.5187, Accuracy: 41.70%\n",
      "Validation Loss: 1.4785, Validation Accuracy: 42.98%\n",
      "--------------------------------------------------\n",
      "Epoch [12/30], Loss: 1.5140, Accuracy: 42.04%\n",
      "Validation Loss: 1.4689, Validation Accuracy: 44.11%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 44.11%\n",
      "Epoch [13/30], Loss: 1.5126, Accuracy: 42.06%\n",
      "Validation Loss: 1.4709, Validation Accuracy: 43.57%\n",
      "--------------------------------------------------\n",
      "Epoch [14/30], Loss: 1.5181, Accuracy: 41.65%\n",
      "Validation Loss: 1.5222, Validation Accuracy: 41.34%\n",
      "--------------------------------------------------\n",
      "Epoch [15/30], Loss: 1.5158, Accuracy: 42.18%\n",
      "Validation Loss: 1.4655, Validation Accuracy: 43.26%\n",
      "--------------------------------------------------\n",
      "Epoch [16/30], Loss: 1.5228, Accuracy: 41.85%\n",
      "Validation Loss: 1.4918, Validation Accuracy: 42.58%\n",
      "--------------------------------------------------\n",
      "Epoch [17/30], Loss: 1.5266, Accuracy: 41.50%\n",
      "Validation Loss: 1.4831, Validation Accuracy: 43.17%\n",
      "--------------------------------------------------\n",
      "Epoch [18/30], Loss: 1.5203, Accuracy: 42.12%\n",
      "Validation Loss: 1.5011, Validation Accuracy: 42.44%\n",
      "--------------------------------------------------\n",
      "Epoch [19/30], Loss: 1.5229, Accuracy: 41.45%\n",
      "Validation Loss: 1.4945, Validation Accuracy: 42.20%\n",
      "--------------------------------------------------\n",
      "Epoch [20/30], Loss: 1.5164, Accuracy: 41.58%\n",
      "Validation Loss: 1.5367, Validation Accuracy: 42.95%\n",
      "--------------------------------------------------\n",
      "Epoch [21/30], Loss: 1.5151, Accuracy: 41.98%\n",
      "Validation Loss: 1.4782, Validation Accuracy: 42.96%\n",
      "--------------------------------------------------\n",
      "Epoch [22/30], Loss: 1.5210, Accuracy: 41.41%\n",
      "Validation Loss: 1.4663, Validation Accuracy: 43.19%\n",
      "--------------------------------------------------\n",
      "Epoch [23/30], Loss: 1.5259, Accuracy: 41.48%\n",
      "Validation Loss: 1.4643, Validation Accuracy: 43.14%\n",
      "--------------------------------------------------\n",
      "Epoch [24/30], Loss: 1.5195, Accuracy: 41.63%\n",
      "Validation Loss: 1.4901, Validation Accuracy: 42.42%\n",
      "--------------------------------------------------\n",
      "Epoch [25/30], Loss: 1.5201, Accuracy: 41.88%\n",
      "Validation Loss: 1.4674, Validation Accuracy: 43.56%\n",
      "--------------------------------------------------\n",
      "Epoch [26/30], Loss: 1.5179, Accuracy: 41.96%\n",
      "Validation Loss: 1.4928, Validation Accuracy: 42.42%\n",
      "--------------------------------------------------\n",
      "Epoch [27/30], Loss: 1.5189, Accuracy: 41.65%\n",
      "Validation Loss: 1.4711, Validation Accuracy: 42.77%\n",
      "--------------------------------------------------\n",
      "Epoch [28/30], Loss: 1.5212, Accuracy: 41.88%\n",
      "Validation Loss: 1.4967, Validation Accuracy: 41.50%\n",
      "--------------------------------------------------\n",
      "Epoch [29/30], Loss: 1.5129, Accuracy: 41.88%\n",
      "Validation Loss: 1.4699, Validation Accuracy: 43.85%\n",
      "--------------------------------------------------\n",
      "Epoch [30/30], Loss: 1.5254, Accuracy: 41.74%\n",
      "Validation Loss: 1.4760, Validation Accuracy: 42.79%\n",
      "--------------------------------------------------\n",
      "Training complete. Best validation accuracy: 44.11%\n",
      "------- Evaluating -------\n",
      "42.90888826971301\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Load pretrained MobileNetV2 model\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Freeze all convolutional layers \n",
    "for param in mobilenet_v2.parameters():\n",
    "    param.requires_grad = False  # Freeze the convolutional layers\n",
    "\n",
    "# Modify the final fully connected layer (classifier) to match emotion classes (FER2013)\n",
    "mobilenet_v2.classifier[1] = nn.Linear(mobilenet_v2.classifier[1].in_features, 7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobilenet_v2 = mobilenet_v2.to(device)\n",
    "\n",
    "# loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mobilenet_v2.classifier[1].parameters(), lr=0.001)\n",
    "\n",
    "train_model(mobilenet_v2, train_loader, val_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "mobilenet_v2_accuracy = evaluate_model(mobilenet_v2, test_loader)\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(mobilenet_v2.state_dict(), \"emotion_recognition_mobilenet_v2.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Training ----------------\n",
      "Epoch [1/30], Loss: 1.5990, Accuracy: 37.37%\n",
      "Validation Loss: 1.5118, Validation Accuracy: 40.98%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 40.98%\n",
      "Epoch [2/30], Loss: 1.5080, Accuracy: 41.88%\n",
      "Validation Loss: 1.5011, Validation Accuracy: 42.96%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 42.96%\n",
      "Epoch [3/30], Loss: 1.4816, Accuracy: 42.88%\n",
      "Validation Loss: 1.4816, Validation Accuracy: 43.16%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.16%\n",
      "Epoch [4/30], Loss: 1.4746, Accuracy: 43.36%\n",
      "Validation Loss: 1.4620, Validation Accuracy: 43.78%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.78%\n",
      "Epoch [5/30], Loss: 1.4778, Accuracy: 43.45%\n",
      "Validation Loss: 1.4650, Validation Accuracy: 43.94%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 43.94%\n",
      "Epoch [6/30], Loss: 1.4704, Accuracy: 43.56%\n",
      "Validation Loss: 1.4551, Validation Accuracy: 44.17%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 44.17%\n",
      "Epoch [7/30], Loss: 1.4612, Accuracy: 43.83%\n",
      "Validation Loss: 1.4552, Validation Accuracy: 44.34%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 44.34%\n",
      "Epoch [8/30], Loss: 1.4627, Accuracy: 43.61%\n",
      "Validation Loss: 1.4561, Validation Accuracy: 44.27%\n",
      "--------------------------------------------------\n",
      "Epoch [9/30], Loss: 1.4663, Accuracy: 43.89%\n",
      "Validation Loss: 1.4567, Validation Accuracy: 44.27%\n",
      "--------------------------------------------------\n",
      "Epoch [10/30], Loss: 1.4656, Accuracy: 43.60%\n",
      "Validation Loss: 1.4465, Validation Accuracy: 44.79%\n",
      "--------------------------------------------------\n",
      "New best model saved with validation accuracy: 44.79%\n",
      "Epoch [11/30], Loss: 1.4668, Accuracy: 43.60%\n",
      "Validation Loss: 1.4621, Validation Accuracy: 43.99%\n",
      "--------------------------------------------------\n",
      "Epoch [12/30], Loss: 1.4656, Accuracy: 43.81%\n",
      "Validation Loss: 1.4524, Validation Accuracy: 44.72%\n",
      "--------------------------------------------------\n",
      "Epoch [13/30], Loss: 1.4618, Accuracy: 43.82%\n",
      "Validation Loss: 1.4459, Validation Accuracy: 44.51%\n",
      "--------------------------------------------------\n",
      "Epoch [14/30], Loss: 1.4577, Accuracy: 44.05%\n",
      "Validation Loss: 1.4474, Validation Accuracy: 44.67%\n",
      "--------------------------------------------------\n",
      "Epoch [15/30], Loss: 1.4578, Accuracy: 43.81%\n",
      "Validation Loss: 1.4465, Validation Accuracy: 44.34%\n",
      "--------------------------------------------------\n",
      "Epoch [16/30], Loss: 1.4602, Accuracy: 43.99%\n",
      "Validation Loss: 1.4551, Validation Accuracy: 44.50%\n",
      "--------------------------------------------------\n",
      "Epoch [17/30], Loss: 1.4528, Accuracy: 43.97%\n",
      "Validation Loss: 1.4477, Validation Accuracy: 44.62%\n",
      "--------------------------------------------------\n",
      "Epoch [18/30], Loss: 1.4630, Accuracy: 43.73%\n",
      "Validation Loss: 1.4556, Validation Accuracy: 44.39%\n",
      "--------------------------------------------------\n",
      "Epoch [19/30], Loss: 1.4610, Accuracy: 43.79%\n",
      "Validation Loss: 1.4537, Validation Accuracy: 44.39%\n",
      "--------------------------------------------------\n",
      "Epoch [20/30], Loss: 1.4628, Accuracy: 43.83%\n",
      "Validation Loss: 1.4526, Validation Accuracy: 44.44%\n",
      "--------------------------------------------------\n",
      "Epoch [21/30], Loss: 1.4553, Accuracy: 43.53%\n",
      "Validation Loss: 1.4607, Validation Accuracy: 44.20%\n",
      "--------------------------------------------------\n",
      "Epoch [22/30], Loss: 1.4585, Accuracy: 44.24%\n",
      "Validation Loss: 1.4694, Validation Accuracy: 43.17%\n",
      "--------------------------------------------------\n",
      "Epoch [23/30], Loss: 1.4543, Accuracy: 44.09%\n",
      "Validation Loss: 1.4613, Validation Accuracy: 44.32%\n",
      "--------------------------------------------------\n",
      "Epoch [24/30], Loss: 1.4596, Accuracy: 43.72%\n",
      "Validation Loss: 1.4611, Validation Accuracy: 43.89%\n",
      "--------------------------------------------------\n",
      "Epoch [25/30], Loss: 1.4616, Accuracy: 43.68%\n",
      "Validation Loss: 1.4653, Validation Accuracy: 43.97%\n",
      "--------------------------------------------------\n",
      "Epoch [26/30], Loss: 1.4680, Accuracy: 43.56%\n",
      "Validation Loss: 1.4595, Validation Accuracy: 44.39%\n",
      "--------------------------------------------------\n",
      "Epoch [27/30], Loss: 1.4566, Accuracy: 43.95%\n",
      "Validation Loss: 1.4607, Validation Accuracy: 43.56%\n",
      "--------------------------------------------------\n",
      "Epoch [28/30], Loss: 1.4642, Accuracy: 43.78%\n",
      "Validation Loss: 1.4578, Validation Accuracy: 44.62%\n",
      "--------------------------------------------------\n",
      "Epoch [29/30], Loss: 1.4601, Accuracy: 43.72%\n",
      "Validation Loss: 1.4808, Validation Accuracy: 43.23%\n",
      "--------------------------------------------------\n",
      "Epoch [30/30], Loss: 1.4566, Accuracy: 43.83%\n",
      "Validation Loss: 1.4546, Validation Accuracy: 44.64%\n",
      "--------------------------------------------------\n",
      "Training complete. Best validation accuracy: 44.79%\n",
      "------- Evaluating -------\n",
      "44.441348565059904\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained EfficientNetB0 model\n",
    "efficientnet_b0 = models.efficientnet_b0(weights='DEFAULT')\n",
    "\n",
    "# Freeze all convolutional layers\n",
    "for param in efficientnet_b0.parameters():\n",
    "    param.requires_grad = False  # Freeze the convolutional layers\n",
    "\n",
    "# Modify the final fully connected layer to match emotion classes (FER2013)\n",
    "efficientnet_b0.classifier[1] = nn.Linear(efficientnet_b0.classifier[1].in_features, 7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientnet_b0 = efficientnet_b0.to(device)\n",
    "\n",
    "# loss function and optimizer (only train the final FC layer)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(efficientnet_b0.classifier[1].parameters(), lr=0.001)\n",
    "\n",
    "train_model(efficientnet_b0, train_loader, val_loader, criterion, optimizer, num_epochs=30)\n",
    "efficientnet_b0_accuracy = evaluate_model(efficientnet_b0, test_loader)\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(efficientnet_b0.state_dict(), \"emotion_recognition_efficientnet_b0.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models on RAF-DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resnet18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23908\\1587554998.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model resnet18 loaded successfully!\n",
      "Loading resnet34...\n",
      "Model resnet34 loaded successfully!\n",
      "Loading mobilenet_v2...\n",
      "Model mobilenet_v2 loaded successfully!\n",
      "Loading efficientnet_b0...\n",
      "Model efficientnet_b0 loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18, resnet34, mobilenet_v2, efficientnet_b0\n",
    "from torchvision.models import ResNet18_Weights, ResNet34_Weights\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def load_model(model_arch,model_path, num_classes, device):\n",
    "    if model_arch == \"resnet18\":\n",
    "        model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif model_arch == \"resnet34\":\n",
    "        model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif model_arch == \"mobilenet_v2\":\n",
    "        model = mobilenet_v2(weights='DEFAULT')\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    elif model_arch == \"efficientnet_b0\":\n",
    "        model = efficientnet_b0(weights='DEFAULT')\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model architecture: {model_arch}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "# Paths\n",
    "model_paths = {\n",
    "    \"resnet18\": r\"C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\User-perception sub-system\\emotion_recognition_resnet18.pth\",\n",
    "    \"resnet34\": r\"C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\User-perception sub-system\\emotion_recognition_resnet34.pth\",\n",
    "    \"mobilenet_v2\": r\"C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\User-perception sub-system\\emotion_recognition_mobilenet_v2.pth\",\n",
    "    \"efficientnet_b0\": r\"C:\\Users\\Usuario\\Uppsala\\IIS\\Project\\User-perception sub-system\\emotion_recognition_efficientnet_b0.pth\",\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(full_dataset_raf.classes) \n",
    "\n",
    "for model_arch, model_path in model_paths.items():\n",
    "    print(f\"Loading {model_arch}...\")\n",
    "    model = load_model(model_arch, model_path, num_classes, device)\n",
    "    print(f\"Model {model_arch} loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating resnet18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23908\\1587554998.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of resnet18 on the test dataset: 40.94%\n",
      "Evaluating resnet34...\n",
      "Accuracy of resnet34 on the test dataset: 43.08%\n",
      "Evaluating mobilenet_v2...\n",
      "Accuracy of mobilenet_v2 on the test dataset: 42.91%\n",
      "Evaluating efficientnet_b0...\n",
      "Accuracy of efficientnet_b0 on the test dataset: 44.44%\n"
     ]
    }
   ],
   "source": [
    "for model_arch, model_path in model_paths.items():\n",
    "    print(f\"Evaluating {model_arch}...\")\n",
    "    model = load_model(model_arch, model_path, num_classes, device)\n",
    "    accuracy = evaluate_model(model, test_loader_raf, device)\n",
    "    print(f\"Accuracy of {model_arch} on the test dataset: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
